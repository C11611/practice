{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、Installation the packages\n",
    "        1、NlP and its importance\n",
    "        2、Analyzing text\n",
    "        3、building NLP applications\n",
    "        4、Extractiong meaningful information from given text data\n",
    "        5、需要安装package:NLP和 gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 tokenizing the input text using nltk\n",
    "        1、need to break it down into smaller pieces\n",
    "        2、tokenization:\n",
    "                    a process of dividing the input text\n",
    "                    b into a set of pieces like words or sentences\n",
    "        3、Pieces are called tokens\n",
    "        4、Divide the text into tokens depending on the purpose\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenzizer:\n",
      "['do you know how tokenziation works?', \"it's actually quite interesting!\", \"let's analyze a couple of sentences and figure it out\"]\n",
      "\n",
      "Word tokenizer:\n",
      "['do', 'you', 'know', 'how', 'tokenziation', 'works', '?', 'it', \"'s\", 'actually', 'quite', 'interesting', '!', 'let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out']\n",
      "\n",
      "word punct tokenizer:\n",
      "['do', 'you', 'know', 'how', 'tokenziation', 'works', '?', 'it', \"'\", 's', 'actually', 'quite', 'interesting', '!', 'let', \"'\", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize,WordPunctTokenizer\n",
    "\n",
    "#Define input text\n",
    "input_text=\"do you know how tokenziation works? it's actually quite interesting! let's analyze a couple of sentences and figure it out \"\n",
    "\n",
    "#sentence tokenizer\n",
    "print(\"\\nSentence tokenzizer:\")\n",
    "print(sent_tokenize(input_text))\n",
    "\n",
    "#word tokenizer\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))\n",
    "\n",
    "#wordpunct tokenizer\n",
    "print(\"\\nword punct tokenizer:\")\n",
    "print(WordPunctTokenizer().tokenize(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、converting words to their base forms\n",
    "        process of stemming:\n",
    "            1. reduce words from different forms into base form\n",
    "            2.cuts off the ends of words to extract base forms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
      " ====================================================================\n",
      "         writing           write            writ           write\n",
      "          calves            calv            calv            calv\n",
      "              be              be              be              be\n",
      "         branded           brand           brand           brand\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl            poss         possibl\n",
      "       provision          provis          provid          provis\n",
      "      hospitital        hospitit        hospitit        hospitit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchy        scratchi\n",
      "            code            code             cod            code\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "input_words=['writing','calves','be','branded','horse','randomize','possibly','provision','hospitital','kept',\n",
    "             'scratchy','code']\n",
    "\n",
    "# create various stemmer objects\n",
    "porter=PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "snowball=SnowballStemmer('english')\n",
    "\n",
    "#create a list of stemmer names for display\n",
    "stemmer_names=['PORTER','LANCASTER','SNOWBALL']\n",
    "formatted_text='{:>16}'*(len(stemmer_names)+1)\n",
    "print('\\n',formatted_text.format('INPUT WORD',*stemmer_names),'\\n','='*68)\n",
    "\n",
    "#stem each word and display the output\n",
    "for word in input_words:\n",
    "    output=[word,porter.stem(word),lancaster.stem(word),snowball.stem(word),lancaster.stem(word),snowball.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、dividing text data into chunks\n",
    "        diving text data into pieces for further analysis\n",
    "        used frequently in text analysis\n",
    "        conditions vary on need\n",
    "        different from tokenization:\n",
    "            no adherence to any constraints\n",
    "            output chunks need to be meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of text chunks= 1 \n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "# split the input text into chunks,where\n",
    "#each chunk contains N words\n",
    "\n",
    "def chunker(input_data,N):\n",
    "    input_words=input_data.split(' ')\n",
    "    output=[]\n",
    "    \n",
    "    cur_chunk=[]\n",
    "    count=0\n",
    "    for word in input_words:\n",
    "        cur_chunk.append(word)\n",
    "        count+=1\n",
    "        if count==N:\n",
    "            output.append(' '.join(cur_chunk))\n",
    "            count,cur_chunk=0,[]\n",
    "            \n",
    "    output.append(' '.join(cur_chunk))\n",
    "    \n",
    "    return output\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #read the first 12000 words form the brown corpus\n",
    "    input_data=''.join(brown.words()[:12000])\n",
    "#     print(input_data)  \n",
    "    \n",
    "    #define the number of words in each chunk\n",
    "    chunk_size=700\n",
    "    \n",
    "    chunks=chunker(input_data,chunk_size)\n",
    "    print('\\nNumber of text chunks=',len(chunks),'\\n')\n",
    "    for i,chunk in enumerate(chunks):\n",
    "         print(i)\n",
    "#         print('Chink',i+1,'==>',chunk[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
