{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1-introduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   0.00000000e+00  -4.44089210e-16   2.22044605e-16]\n",
      " [  1.11022302e-16   1.00000000e+00   0.00000000e+00   4.44089210e-16]\n",
      " [  0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   4.44089210e-16   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import*\n",
    "#random.rand(4,4)\n",
    "randmat=mat(random.rand(4,4))\n",
    "invRandMat=randmat.I\n",
    "print(randmat*invRandMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chpter 2-KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn-Classifying with k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of time spent playing video games? 0.001\n",
      "frequent flier miles earned per year?  0.001\n",
      "liters of ice cream consumed per year? 0.001\n",
      "{'2': 1}\n",
      "{'2': 2}\n",
      "{'2': 3}\n",
      "You will probably like this person:  in small doses\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy import *\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#define function to generate labels and group\n",
    "def createDataSet():\n",
    "    group=array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])\n",
    "    labels=['A','A','B','B']\n",
    "    return group,labels\n",
    "\n",
    "#define function to classify the object\n",
    "def classify0(inX,dataSet,labels,k):\n",
    "    #Distance calculation\n",
    "    dataSetSize=dataSet.shape[0] # get the rows of data\n",
    "    diffMat=tile(inX,(dataSetSize,1))-dataSet  # extend the data to the same rows of data  and compute the cost function\n",
    "    sqDiffMat=diffMat**2\n",
    "    sqDistances=sqDiffMat.sum(axis=1)# to sum by the row\n",
    "    distances=sqDistances**0.5\n",
    "    sortedDistIndicies=distances.argsort() # important how to use argsort to return the index of elements(from small to bigger)\n",
    "    #print(len(sortedDistIndicies))\n",
    "    #print(\"the k is %s\"%k)\n",
    "    #print(\"the labels is %s\" %labels)\n",
    "    #Voting with lowest  k distances \n",
    "    #count the lowest k distances\n",
    "    classCount={}\n",
    "    for i in range(k):\n",
    "        voteIlabel=labels[sortedDistIndicies[i]]\n",
    "        classCount[voteIlabel]=classCount.get(voteIlabel,0)+1\n",
    "        print(classCount)\n",
    "        \n",
    "    #Sort dictionary\n",
    "    sortedClassCount=sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)# reverse is true to oreder by desc \n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "    \n",
    "    \n",
    "#define functrion to parse files\n",
    "def file2matrix(filename,dim2):\n",
    "    fr=open(filename)\n",
    "    #Get number of lines in file\n",
    "    numberOfLines=len(fr.readlines())\n",
    "    #Create NumPy  matrix to return\n",
    "    returnMat=zeros((numberOfLines,dim2))\n",
    "    classLabelVector=[]\n",
    "    fr=open(filename)\n",
    "    index=0\n",
    "    #Parse line to a list\n",
    "    for line in fr.readlines():\n",
    "        line=line.strip()\n",
    "        listFromLine=line.split('\\t')\n",
    "        returnMat[index,:]=listFromLine[0:dim2]\n",
    "        classLabelVector.append(str(listFromLine[-1]))\n",
    "        index+=1\n",
    "       # if index==1:\n",
    "      #      print(listFromLine)\n",
    "    return returnMat,classLabelVector\n",
    "    \n",
    "    \n",
    "#define function to normalize the data\n",
    "def autoNorm(dataSet):\n",
    "    minVals=dataSet.min(0)\n",
    "    maxVals=dataSet.max(0)\n",
    "    ranges=maxVals-minVals\n",
    "    normDataSet=zeros(shape(dataSet))\n",
    "    m=dataSet.shape[0]\n",
    "    normDataSet=dataSet-tile(minVals,(m,1))\n",
    "    normDataSet=normDataSet/tile(ranges,(m,1))\n",
    "    return normDataSet,ranges,minVals\n",
    "\n",
    "#Classifier testing code for dating site\n",
    "\n",
    "def datingClassTest():\n",
    "    hoRatio=0.20\n",
    "    datingDataMat,datingLabels=file2matrix('data/datingTestSet2.txt',3)\n",
    "    normMat,ranges,minVals=autoNorm(datingDataMat)\n",
    "    m=normMat.shape[0]\n",
    "    print(m)\n",
    "    numTestVecs=int(m*hoRatio)\n",
    "    print(numTestVecs)\n",
    "    errorCount=0.0\n",
    "    datingLabels=array(datingLabels)\n",
    "    #print(datingLabels)##add to test the performance\n",
    "    datingLabels=datingLabels.astype('float64')\n",
    "    for i in range(numTestVecs):\n",
    "        classifierResult=classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3)\n",
    "        #print(classifierResult)\n",
    "        print(\"the classifier came back with:%d,the real anser is:%d\"%(classifierResult,datingLabels[i]))\n",
    "        if (classifierResult!=datingLabels[i]):\n",
    "            errorCount+=1.0\n",
    "        print(\"the total error rate is:%f\"%(errorCount/float(numTestVecs)))\n",
    "        \n",
    "        \n",
    "        \n",
    "#Dating site predictor function\n",
    "def classifyPerson():\n",
    "    resultList=['not at all','in small doses','in large doses']\n",
    "    percentTats=float(input(\"percentage of time spent playing video games? \"))\n",
    "    ffMiles=float(input(\"frequent flier miles earned per year?  \"))\n",
    "    iceCream=float(input(\"liters of ice cream consumed per year? \"))\n",
    "    datingDataMat,datingLabels=file2matrix('data/datingTestSet2.txt',3)\n",
    "    #print(\"the datingLabels is %s\"%datingLabels)\n",
    "    normMat,ranges,minVals=autoNorm(datingDataMat)\n",
    "    inArr=array([ffMiles,percentTats,iceCream])\n",
    "    classifierResult=classify0((inArr-minVals)/ranges,normMat,datingLabels,3)\n",
    "    #print(classifierResult)\n",
    "    print(\"You will probably like this person: \",resultList[int(classifierResult)-1])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    datingDataMat,datingLabels=file2matrix('data/datingTestSet2.txt',3)\n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(211)\n",
    "    labels=array(datingLabels)\n",
    "    labels=labels.astype('float64')# to avoid error in scatter\n",
    "    #datingClassTest()\n",
    "    classifyPerson()\n",
    "#    plt.title(\"Icecreame and videogames\")\n",
    "#    plt.xlabel(\"Liters of Ice Cream Consumed Per Week\")\n",
    "#    plt.ylabel(\"Percentage of Time Spent Playing Video Games\")\n",
    "#    ax.scatter(datingDataMat[:,1],datingDataMat[:,2],15.0*labels,15.0*labels)\n",
    " \n",
    "    \n",
    " #   ax=fig.add_subplot(2,2,2)\n",
    "#    plt.title(\"Icecreame and videogames\")\n",
    "#    plt.xlabel(\"Percentage of Time Spent Playing Video Games\")\n",
    "#    plt.ylabel(\"frequent fLYIER miles Earned Per Year\")\n",
    "#    ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*labels,15.0*labels)\n",
    "#   plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3\n",
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-1b2b0279d018>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-1b2b0279d018>\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    calssCount{}\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries=len(dataSet)\n",
    "    labelCounts={}\n",
    "    for featVec in dataSet:  #for each element in the dataSet,choose the last value \n",
    "        currentLabel=featVec[-1]  \n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel]=0\n",
    "            labelCounts[currentLabel]+=1\n",
    "    shannonEnt=0.0\n",
    "    for key in labelCounts:\n",
    "        prob=float(labelCounts[key])/numEntries\n",
    "        shannonEnt-=prob*log(prob,2)\n",
    "    return shannonEnt\n",
    "\n",
    "def createDataSet():\n",
    "    dataSet=[['1','1','yes'],['1','1','yes'],['1','0','no'],['0','1','no'],['0','1','no']]\n",
    "    labels=['no surfacing','flippers']\n",
    "    return dataSet,labels\n",
    "\n",
    "def splitDataSet(dataSet,axis,value):\n",
    "    retDataSet=[]\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis]==value:\n",
    "            reducedFeatVec=featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures=len(dataSet[0])-1\n",
    "    baseEntrop=calcShannonEnt(dataSet)\n",
    "    bestInfoGain=0.0;bestFeature=-1\n",
    "    for i in range(numFeatures):\n",
    "        featList=[example[i] for example in dataSet]\n",
    "        uniqueVals=set(featList)\n",
    "        newEntrop=0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet=splitDataSet(dataSet,i,value)\n",
    "            prob=len(subDataSet)/float(len(dataSet))\n",
    "            newEntrop+=prob*calcShannonEnt(subDataSet)\n",
    "        infoGain=baseEntrop-newEntrop\n",
    "        if (infoGain>bestInfoGain):\n",
    "            bestInfoGain=infoGain\n",
    "            bestFeature=i  ####?????????\n",
    "    return bestFeature\n",
    "\n",
    "\n",
    "\n",
    "def majorityCnt(classlist):\n",
    "    calssCount{}\n",
    "    for vote in classList:\n",
    "        if vote not in calssCount.keys()\n",
    "        clasCount[vote]+=1\n",
    "    sortedClassCount=sorted(classCount.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "\n",
    "def createTree(dataSet,labels):\n",
    "    classList=[example[-1] for example in dataSet]\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat=chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLable=labels[bestFeat]\n",
    "    del(labels[bestFeat])\n",
    "    featValues=[example[bestFeature] for example in dataSet]\n",
    "    uniqueVals=set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels=labels[:]\n",
    "        myTress[bestFeatLabel][value]=createTree(splitDataSet\\\n",
    "                                                 (dataSet,bestFeat,value),subLabels)\n",
    "    return myTress\n",
    "\n",
    "    \n",
    "            \n",
    "if __name__==\"__main__\":\n",
    "    myDat,labels=createDataSet()\n",
    "    print(myDat)\n",
    "    print(calcShannonEnt(myDat))\n",
    "    print(chooseBestFeatureToSplit(myDat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "-------\n",
      "[1, 'yes']\n",
      "--------\n",
      "[[1, 'yes']]\n",
      "[1]\n",
      "-------\n",
      "[1, 'yes']\n",
      "--------\n",
      "[[1, 'yes'], [1, 'yes']]\n",
      "[0]\n",
      "-------\n",
      "[0, 'no']\n",
      "--------\n",
      "[[1, 'yes'], [1, 'yes'], [0, 'no']]\n",
      "[0]\n",
      "-------\n",
      "[0, 'no']\n",
      "--------\n",
      "[[1, 'yes'], [1, 'yes'], [0, 'no'], [0, 'no']]\n",
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "[[1, 'yes'], [1, 'yes'], [0, 'no'], [0, 'no']]\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries=len(dataSet)\n",
    "    \n",
    "    labelCounts={}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel=featVec[-1] \n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel]=0\n",
    "            #print(labelCounts[currentLabel])\n",
    "        labelCounts[currentLabel]+=1\n",
    "        #print(labelCounts[currentLabel])\n",
    "    #print(labelCounts)\n",
    "    shannonEnt=0.0\n",
    "    for key in labelCounts:\n",
    "        prob=float(labelCounts[key])/numEntries\n",
    "        shannonEnt-=prob*log(prob,2)\n",
    "    return shannonEnt\n",
    "\n",
    "def createDataSet():\n",
    "    dataSet=[[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']]\n",
    "    labels=['no surfacing','flippers']\n",
    "    return dataSet,labels\n",
    "\n",
    "def splitDataSet(dataSet,axis,value):\n",
    "    retDataSet=[]\n",
    "    for featVec in dataSet:\n",
    "        reducedFeatVec=[]\n",
    "        if featVec[axis]==value:\n",
    "            reducedFeatVec=featVec[:axis]\n",
    "            print(reducedFeatVec)\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            print(\"-------\")\n",
    "            print(reducedFeatVec)\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "            print(\"--------\")\n",
    "            print(retDataSet)\n",
    "    return retDataSet\n",
    "\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures=len(dataSet[0])-1\n",
    "    #print(numFeatures)\n",
    "    baseEntrop=calcShannonEnt(dataSet)\n",
    "    #print(baseEntrop)\n",
    "    bestInfoGain=0.0;bestFeature=-1  #why define the bestFeature -1\n",
    "    for i in range(numFeatures):\n",
    "        featList=[example[i] for example in dataSet]\n",
    "        print(featList)\n",
    "        uniqueVals=set(featList)\n",
    "        print(uniqueVals)\n",
    "        newEntrop=0.0\n",
    "        #caculate the entrop of the tree\n",
    "        for value in uniqueVals:\n",
    "            subDataSet=splitDataSet(dataSet,i,value)\n",
    "            print(subDataSet)\n",
    "            prob=len(subDataSet)/float(len(dataSet))\n",
    "            print(prob)\n",
    "            newEntrop+=prob*calcShannonEnt(subDataSet)\n",
    "            print(newEntrop)\n",
    "        infoGain=baseEntrop-newEntrop\n",
    "        print(\"infoGain: %s\"%infoGain)\n",
    "        if (infoGain>bestInfoGain):\n",
    "            bestInfoGain=infoGain\n",
    "            bestFeature=i  ####?????????\n",
    "    return bestFeature\n",
    "\n",
    "\n",
    "def createTree(dataSet,labels):\n",
    "    classList=[example[-1] for example in dataSet]\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat=chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLable=labels[bestFeat]\n",
    "    del(labels[bestFeat])\n",
    "    featValues=[example[bestFeature] for example in dataSet]\n",
    "    uniqueVals=set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels=labels[:]\n",
    "        myTress[bestFeatLabel][value]=createTree(splitDataSet\\\n",
    "                                                 (dataSet,bestFeat,value),subLabels)\n",
    "    return myTress\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    mydat,labels=createDataSet()\n",
    "    #classList=[example[-1] for example in mydat]\n",
    "    #a=classList.count(classList[0])\n",
    "    #print(classList)\n",
    "    #print(a)\n",
    "    #trees=createTree(mydat,labels)\n",
    "    #mydat[0][-1]=\"maybe\"\n",
    "    #print(mydat)\n",
    "    #ent=calcShannonEnt(mydat)\n",
    "    #print(ent)\n",
    "    reDataSet=splitDataSet(mydat,1,1)\n",
    "    print(mydat)\n",
    "    print(reDataSet)\n",
    "    #bFeature=chooseBestFeatureToSplit(mydat)\n",
    "    #print(bFeature)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=[3,4,2]\n",
    "b=a[:0]\n",
    "b.extend(a[1:])\n",
    "#print(a[1:])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-2\n"
     ]
    }
   ],
   "source": [
    "a=[['5r678','678','-1'],['987654','6578','-2']]\n",
    "for c in a:\n",
    "    b=c[-1]\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "'''''创建我们所要分类的决策树'''  \n",
    "def createTree(dataSet,label):      \n",
    "    classList=[example[-1] for example in dataSet]   #classList是指当前数据集的类别标签  \n",
    "    if classList.count(classList[0])==len(classList): #计算classList中某个类别标签的数量，若只有一类，则数量与它的数据长度相等  \n",
    "        return classList[0]  \n",
    "    if len(dataSet[0])==1:   #当处理完所有特征而类别标签还不唯一时起作用  \n",
    "        return majorityCnt(classList)  \n",
    "    featBest=chooseBestFeature(dataSet)  #选择最好的分类特征  \n",
    "    feature=[example[featBest] for example in dataSet]  #接下来使用该分类特征进行分类  \n",
    "    featValue=set(feature)  #得到该特征所有的分类值，如'0'和'1'  \n",
    "    newLabel=label[featBest]  \n",
    "    del(label[featBest])  \n",
    "    Tree={newLabel:{}}  #创建一个多重字典，存储决策树分类结果  \n",
    "    for value in featValue:  \n",
    "        subLabel=label[:]  \n",
    "        Tree[newLabel][value]=createTree(spiltData(dataSet,featBest,value),subLabel) #递归函数使得Tree不断创建分支，直到分类结束  \n",
    "    return Tree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''''使用决策树执行分类，返回分类结果'''  \n",
    "def classify(tree,label,testVec):       #tree为createTree()函数返回的决策树；label为特征的标签值；testVec为测试数据，即所有特征的具体值构成的向量  \n",
    "    firstFeat=tree.keys()[0]            #取出tree的第一个键  \n",
    "    secondDict=tree[firstFeat]          #取出tree第一个键的值，即tree的第二个字典（包含关系）  \n",
    "    labelIndex=label.index(firstFeat)   #得到第一个特征firstFeat在标签label中的索引  \n",
    "    for key in secondDict.keys():       #遍历第二个字典的键  \n",
    "        if testVec[labelIndex]==key:    #如果第一个特征的测试值与第二个字典的键相等时  \n",
    "            if type(secondDict[key]).__name__=='dict':  #如果第二个字典的值还是一个字典，说明分类还没结束，递归执行classify函数  \n",
    "                classLabel=classify(secondDict[key],label,testVec)  #递归函数中只有输入的第一个参数不同，不断向字典内层渗入  \n",
    "            else:  \n",
    "                classLabel=secondDict[key]  #最后将得到的分类值赋给classLabel输出  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''''使用pickle模块存储决策树'''  \n",
    "def storeTree(tree,filename):    \n",
    "    import pickle  \n",
    "    fw=open(filename,'w')  \n",
    "    pickle.dump(tree,fw)  \n",
    "    fw.close()  \n",
    "  \n",
    "'''''打开文件取出决策树'''  \n",
    "def loadTree(filename):           \n",
    "    import pickle  \n",
    "    fr=open(filename,'r')  \n",
    "    return pickle.load(fr)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter  8  prediction numeric values:regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "def loadDataSet(fileName):\n",
    "    numFeat = len(open(fileName).readline().split('\\t')) - 1\n",
    "    dataMat = []\n",
    "    labelMat = []\n",
    "\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "            lineArr =[]\n",
    "            curLine = line.strip().split('\\t')\n",
    "            for i in range(numFeat):\n",
    "                lineArr.append(float(curLine[i]))\n",
    "            dataMat.append(lineArr)\n",
    "            labelMat.append(float(curLine[-1]))\n",
    "    return dataMat,labelMat\n",
    "\n",
    "def standRegres(xArr,yArr):\n",
    "    xMat = mat(xArr); yMat = mat(yArr).T\n",
    "    xTx = xMat.T*xMat\n",
    "    print(xTx)\n",
    "    print(linalg.det(xTx).I)\n",
    "    if linalg.det(xTx) == 0.0:  # 矩阵的行列式等于0说明,矩阵不可逆\n",
    "        print (\"This matrix is singular, cannot do inverse\")\n",
    "        return\n",
    "    ws = xTx.I * (xMat.T*yMat)\n",
    "    return ws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the price of bosth to do the regression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "theta_g = np.array([1., 1., 1., 1.]) #初始化theta\n",
    "theta_g = theta_g.reshape(4, 1)\n",
    "alpha = 0.1\n",
    "temp = theta_g\n",
    "X0 = X.iloc[:, 0].reshape(150, 1)\n",
    "X1 = X.iloc[:, 1].reshape(150, 1)\n",
    "X2 = X.iloc[:, 2].reshape(150, 1)\n",
    "X3 = X.iloc[:, 3].reshape(150, 1)\n",
    "J = pd.Series(np.arange(800, dtype = float))\n",
    "for i in range(800):\n",
    "# theta j := theta j - alpha*sum((h(xi)-yi)*xi)/m\n",
    "     temp[0] = theta_g[0] + alpha*np.sum((Y- dot(X, theta_g))*X0)/150.\n",
    "     temp[1] = theta_g[1] + alpha*np.sum((Y- dot(X, theta_g))*X1)/150.\n",
    "     temp[2] = theta_g[2] + alpha*np.sum((Y- dot(X, theta_g))*X2)/150.\n",
    "    temp[3] = theta_g[3] + alpha*np.sum((Y- dot(X, theta_g))*X3)/150.\n",
    "    J[i] = 0.5*np.sum((Y - dot(X, theta_g))**2) #计算损失函数值    \n",
    "theta_g = temp #更新theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411856.404013\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/four/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: RuntimeWarning: overflow encountered in power\n",
      "/Users/four/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in multiply\n",
      "/Users/four/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "14\n",
      "[0, 10000, 20000, 30000, 40000]\n",
      "[4028576557461.1162, nan, nan, nan, nan]\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1d8c4ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt  \n",
    "  \n",
    "  \n",
    "  \n",
    "#  \n",
    "#        批量梯度下降  \n",
    "#  \n",
    "def gradientDescent(x,y,theta,alpha,iters): #alpha = 学习速率  ，iters = 迭代次数  \n",
    "    temp = np.matrix(np.zeros(theta.shape))  \n",
    "    print(temp)  \n",
    "    parameters = int(theta.ravel().shape[1])  # ravel就是合并矩阵  \n",
    "    print(parameters)  \n",
    "    cost = np.zeros(iters)  \n",
    "    itersNum=[]\n",
    "    cost100=[]\n",
    "    for i in range(iters):  \n",
    "        error = (x * theta.T) - y  \n",
    "        for j in range(parameters):  \n",
    "            term = np.multiply(error,x[:,j])      #multiply 对应元素乘法  \n",
    "            temp[0,j] = theta[0,j] - ((alpha / len(x)) * np.sum(term))  \n",
    "        theta = temp  \n",
    "        cost[i] = computeCost(x,y,theta)\n",
    "        if i%10000==0:\n",
    "            cost100.append(cost[i])\n",
    "            itersNum.append(i)\n",
    "    return theta,cost,itersNum,cost100  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "boston=load_boston()\n",
    "data = pd.DataFrame(boston.data,columns=boston.feature_names) \n",
    "data['price']=boston.target #添加y轴  \n",
    "data.insert(0,'ones',1)    #添加一列 就是在第一列（0） 添加名字为 ones 的一列数据，他的数值都是 1      #偏置数值x0 = 1 ！！！\n",
    "\n",
    " \n",
    "# data.plot(kind = 'scatter' , x = 'population' , y = 'profit' )    #  设置画板类型,figsize是画板大小  \n",
    "# plt.show()  \n",
    "  \n",
    "def computeCost(x,y,theta):   #初始化单变量线性回归  \n",
    "    inner = np.power(((x*theta.T) - y),2)     #power(x,2)  , 就是将x数组里面的元素都2次方  \n",
    "    return np.sum(inner) / (2*len(x)) \n",
    "\n",
    "#初始化变量  \n",
    "cols = data.shape[1]  \n",
    "x = data.iloc[:,0:cols - 1]    #x是所有的行 ， 去掉最后一列  \n",
    "y = data.iloc[:,cols - 1:cols]   # y就是最后一列数据  \n",
    "  \n",
    "  \n",
    "#初始化数据traning数据  \n",
    "x = np.matrix(x.values)   #转化成 矩阵形式  \n",
    "y = np.matrix(y)  \n",
    "theta = np.matrix(np.ones(cols - 1)) #theta 初始化  \n",
    "costFunction = computeCost(x,y,theta)  \n",
    "print(costFunction) \n",
    "  \n",
    "#初始化数据  \n",
    "alpha = 0.01  \n",
    "iters = 50000  \n",
    "gradientDescent(x,y,theta,alpha,iters)  \n",
    "g,cost,num,cost10 = gradientDescent(x,y,theta,alpha,iters)  \n",
    "print(num)\n",
    "print(cost10)\n",
    "print(computeCost(x,y,g)) \n",
    "plt.plot(num,cost10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#绘制拟合曲线  \n",
    "# x = np.linspace(data.population.min(),data.population.max(),100)  #llinspace(start,stop,num) num就是生成的样本数  \n",
    "# f = g[0,0] + (g[0,1] * x)   #g[0,0] 代表theta0 , g[0,1] 代表theta1  \n",
    "# fig,ax = plt.subplots()  \n",
    "# ax.plot(x,f,'r',label = 'prediction')  \n",
    "# ax.scatter(data.population,data.profit,label = 'training data')  \n",
    "# ax.legend(loc=2)  \n",
    "# ax.set_xlabel('population')  \n",
    "# ax.set_ylabel('profit')  \n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "open() argument 2 must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0818390836e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/datingTestSet2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: open() argument 2 must be str, not int"
     ]
    }
   ],
   "source": [
    "rows=open(\"data/datingTestSet2.txt\",3)\n",
    "row\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
